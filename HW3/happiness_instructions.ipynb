{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgustinaLorda/AgustinaLorda/blob/main/HW3/happiness_instructions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW 2 Assignment 2:\n",
        "\n",
        "Following the work we did together on the youtube videos and movies datasets, explore this dataset of \"world happiness\".\n",
        "This is a free form assignment: all the tasks have been performed in the _youtube videos_ or _movies_ data exploration we did in class (see class lectures and associated notebooks, also see Assignment 1 in HW 2).\n",
        "\n",
        "Find the datast happines.csv in\n",
        "https://github.com/fedhere/FDSfE_FBianco"
      ],
      "metadata": {
        "id": "j5k4PjJ-w0hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "On your own, perform the following tasks and report the results (with neat print statemnets as done in https://github.com/fedhere/FDSfE_FBianco/blob/main/HW2/movies_exploratory_instructions.ipynb, and remember that _all_ figures need axis labels and figure captions).\n",
        "\n",
        "1. find the shape of the dataframe\n",
        "2. show the top _10 rows_ and bottom _10 rows_ of the dataframe (_done in the videos and movies exploration_)\n",
        "3. find statistical properties of the numerical columns of the dataframe (mean, standard deviation, quantiles etc)  (_done in the videos and movies exploration_)\n",
        "4. inspect if the dataframe has missing values (_done in the movies exploration_). If it does have any decide how to deal with them: you can remove the rows or columns with missing values (with _drop_ or _dropna_ as we did in the movies assignment, but make sure you do not need them to answer the following questions before you drop them!), or replace them with some value that you thin is appropriate, for example with with _fillna_, see instructions on this function [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)). The choice of how to deal with missing values is yours, but you do have to end up with a dataframe with no missing values (make sure you show that the resulting dataframe has no missing values either with a printed output or with a plot).\n",
        "5. find the happiest country.\n",
        "6. is the happiest country also the top country in any of the following variables: [Economy (GDP per Capita),\tFamily\tHealth, (Life Expectancy),\tFreedom\tTrust (Government Corruption),Generosity\tDystopia Residual]\n",
        "\n",
        "7. find the happiness score for a country of your choice, for example your country of origin, or any country you like. We did not do this exact task in either notebook _but_ to do that you can use a condition like `df[\"Happiness Score\"] == \"mycountry\"` - this is similar to what you did to get the maximum value for a country: `df[\"Happiness Score\"] == df[\"Happiness Score\"].max()` and then use this statement as an index for the dataframe).\n",
        "\n",
        "8. find the top 10 happiest coutries (using the `sort_values` function like we did in class for the videos analysis - _done in the videos and movies exploration_)\n",
        "9. measure the correlation among (numerical) variables (_done in the videos and movies exploration_)\n",
        "10. show a plot of the correlation of (numerical) variables (we did it in a couple of different ways in the youtube videos notebook) and describe it in a caption - what do you see? any noticeable correlations or anticorrelations? (_done in the videos and movies exploration_)\n",
        "\n",
        "THE LAST TWO TASKS ARE A BIT HARDER BECAUSE WE DID NOT DO ThEM TOGETHER IN CLASS SO THEY ARE GUIDED BELOW\n",
        "\n",
        "11. aggregate the countries by region with `_group_by()` (_done in the videos and movies exploration_) and find the region with the highest \"Generosity\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "REMEMBER:\n",
        "- for each plot make a caption that states \"what\" and \"why\" (see HW2 assignment 1) and make sure the plots have appropriate axis labels.\n",
        "- for every \"finding\" make sure the result is clear, for example by clearly extracting it in a print statement or by writing a text cell below the core that shows the result\n",
        "- save your notebook on Github in your FDSfE repository as HW1_happiness.ipynb\n"
      ],
      "metadata": {
        "id": "H34AC9ynuRA1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad2kcvGJiJT3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "happiness15 = pd.read_cs...(\"https://raw.githubusercontent.com/fedhere/FDSfE_FBianco/main/data/happines.csv\")"
      ],
      "metadata": {
        "id": "daUcevphi3k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...... do all the tasks through 1-10. You have done before in the previous homework and in class so you should be familiar with them!"
      ],
      "metadata": {
        "id": "c7wQlmZ7puFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 11"
      ],
      "metadata": {
        "id": "YHqWeW_9qSfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the groupby method of a dataframe allows you to group rows based on having the same value for a specified column. It returhs a `groupby object` which is not super helpful. But to that object you can apply methods, such as `mean()`, or `count()` to figure out what is the mean of the rows in that group, or how many objects are in each group (respetively `mean()` and `count()`, but there are many other methods!!).\n",
        "\n",
        "the syntax is\n",
        "`df.groupby(\"column name\").mean()`\n",
        "and\n",
        "`df.groupby(\"column name\").count()`\n",
        "\n",
        "for a dataframe called `df` and the column called \"column name\" you want to base the grouping on\n",
        "\n",
        "When you apply a method to the `groupby object` the return of that method is a dataframe, with the same columns as the original dataframe, but only one row per group and the values in each row-column cell is the value generated by applying the method to that group of rows and that column.\n",
        "\n",
        "Try and get the **mean \"Generosity\" per world region** (adapting the lines of code I gave you above), then sort the dataframe that results from this by \"Generosity\" and find the most ... generose region of the world."
      ],
      "metadata": {
        "id": "yOUOtLMcqVA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The most generous region is\")\n",
        "happiness15..."
      ],
      "metadata": {
        "id": "aYdg6iL28gXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 12"
      ],
      "metadata": {
        "id": "xLDAlxiBsQAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very common test in the Hypothesis Testing (or p-value statistics) framwork is the Kolmogorov-Smornoff test (most commonly referred to as K-S test). Like the Z test we reviewed in class, its just a way to compare or two samples (or a sample and a population, but we are using the two-sample version of the test today) to measure if we can rule out that they are \"the same\", or more precisely, that they were exctracted  from the same population. In other words: **THE NULL HYPOTHESIS IS THAT THE TWO SAMPLES COME FROM THE SAME POPULATION**. I will explain on tuesday exactly how it works from a theoretical persoective (its one of the most intuitive ones because it has a graphical explanation), but I want you to figure out how to use it.\n",
        "\n",
        "The function you want to use is in the package `scipy` subpackage `stats` and its called `ks_2samp()`.\n",
        "\n",
        "Find the function and ues the ? to get the functions' doc string. With that, you can figure out how to use it (whta does it want in input?).\n",
        "\n",
        "Read also what the function returns:\n",
        "\n",
        "the _statistic_ is the value of the quantity you measured, is the same kind of quantity as the Z we calculated for the Z test in class, but for the K-S test.\n",
        "\n",
        "Scipy also conveniently returns the _p-value_: this corresponds to the probability that a result as extreme as the one you got would have been obetained by chance. If that probability is _lower_ than your threshold (which you should have set the second I told you this was a p-value kind of test!!!!! explicitly write your significance threshold before you run the test: 2sigma = 16%, 2sigma = 5%, 3sigma = 0.3%...) *you can reject the Null hypothesis that the distributions are from the same population*.\n",
        "\n",
        "So the task is to\n",
        "- Compare the Generosity distribution of values with themselves (that is, pass happiness15[\"Generosity\"] as both arguments of the function).\n",
        "Think about why you get the result. Does the p-value make sense? explain.\n",
        "\n",
        "- Now look at your correlation results, take the two columns that have the largest correlation, and calculate the KS test for those. What can you say about them? (write it in a cell of code)"
      ],
      "metadata": {
        "id": "MQtWwZ95sTda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from .....\n",
        "stats.ks_2samp?"
      ],
      "metadata": {
        "id": "q-uY2QN-yTBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generosity with itself\n",
        "stats.ks_2samp...."
      ],
      "metadata": {
        "id": "GrMan1cRnU4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the two variable that are most correlated with each other\n",
        "stats.ks_2samp..."
      ],
      "metadata": {
        "id": "zRfRHR1v8qYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWdxM3Pfnb11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}